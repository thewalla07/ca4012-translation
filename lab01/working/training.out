Using SCRIPTS_ROOTDIR: /usr/local/mosesdecoder/scripts
Using single-thread GIZA
(1) preparing corpus @ Tue Feb  7 13:23:19 GMT 2017
Executing: mkdir -p /users/case4/wallm22/smt-lab/working/train/corpus
(1.0) selecting factors @ Tue Feb  7 13:23:19 GMT 2017
(1.1) running mkcls  @ Tue Feb  7 13:23:19 GMT 2017
/usr/local/mosesdecoder/tools/mkcls -c50 -n2 -p/users/case4/wallm22/smt-lab/corpus/train-data-clean.fr -V/users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb.classes opt
Executing: /usr/local/mosesdecoder/tools/mkcls -c50 -n2 -p/users/case4/wallm22/smt-lab/corpus/train-data-clean.fr -V/users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 2416

start-costs: MEAN: 110889 (110640-111138)  SIGMA:249.025   
  end-costs: MEAN: 94773 (94674.3-94871.7)  SIGMA:98.66   
   start-pp: MEAN: 220.711 (216.29-225.132)  SIGMA:4.42082   
     end-pp: MEAN: 60.3545 (59.8755-60.8335)  SIGMA:0.479   
 iterations: MEAN: 59677 (59375-59979)  SIGMA:302   
       time: MEAN: 0.944 (0.851-1.037)  SIGMA:0.093   
(1.1) running mkcls  @ Tue Feb  7 13:23:21 GMT 2017
/usr/local/mosesdecoder/tools/mkcls -c50 -n2 -p/users/case4/wallm22/smt-lab/corpus/train-data-clean.en -V/users/case4/wallm22/smt-lab/working/train/corpus/en.vcb.classes opt
Executing: /usr/local/mosesdecoder/tools/mkcls -c50 -n2 -p/users/case4/wallm22/smt-lab/corpus/train-data-clean.en -V/users/case4/wallm22/smt-lab/working/train/corpus/en.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 2015

start-costs: MEAN: 101024 (100824-101223)  SIGMA:199.295   
  end-costs: MEAN: 87026.7 (87017.9-87035.4)  SIGMA:8.75816   
   start-pp: MEAN: 166.756 (163.864-169.648)  SIGMA:2.89186   
     end-pp: MEAN: 49.3179 (49.2804-49.3555)  SIGMA:0.0375889   
 iterations: MEAN: 46884 (46307-47461)  SIGMA:577   
       time: MEAN: 0.7925 (0.713-0.872)  SIGMA:0.0795   
(1.2) creating vcb file /users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb @ Tue Feb  7 13:23:22 GMT 2017
(1.2) creating vcb file /users/case4/wallm22/smt-lab/working/train/corpus/en.vcb @ Tue Feb  7 13:23:22 GMT 2017
(1.3) numberizing corpus /users/case4/wallm22/smt-lab/working/train/corpus/fr-en-int-train.snt @ Tue Feb  7 13:23:22 GMT 2017
(1.3) numberizing corpus /users/case4/wallm22/smt-lab/working/train/corpus/en-fr-int-train.snt @ Tue Feb  7 13:23:23 GMT 2017
(2) running giza @ Tue Feb  7 13:23:23 GMT 2017
(2.1a) running snt2cooc fr-en @ Tue Feb  7 13:23:23 GMT 2017

Executing: mkdir -p /users/case4/wallm22/smt-lab/working/train/giza.fr-en
Executing: /usr/local/mosesdecoder/tools/snt2cooc.out /users/case4/wallm22/smt-lab/working/train/corpus/en.vcb /users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb /users/case4/wallm22/smt-lab/working/train/corpus/fr-en-int-train.snt > /users/case4/wallm22/smt-lab/working/train/giza.fr-en/fr-en.cooc
/usr/local/mosesdecoder/tools/snt2cooc.out /users/case4/wallm22/smt-lab/working/train/corpus/en.vcb /users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb /users/case4/wallm22/smt-lab/working/train/corpus/fr-en-int-train.snt > /users/case4/wallm22/smt-lab/working/train/giza.fr-en/fr-en.cooc
END.
(2.1b) running giza fr-en @ Tue Feb  7 13:23:23 GMT 2017
/usr/local/mosesdecoder/tools/GIZA++  -CoocurrenceFile /users/case4/wallm22/smt-lab/working/train/giza.fr-en/fr-en.cooc -c /users/case4/wallm22/smt-lab/working/train/corpus/fr-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /users/case4/wallm22/smt-lab/working/train/giza.fr-en/fr-en -onlyaldumps 1 -p0 0.999 -s /users/case4/wallm22/smt-lab/working/train/corpus/en.vcb -t /users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb
Executing: /usr/local/mosesdecoder/tools/GIZA++  -CoocurrenceFile /users/case4/wallm22/smt-lab/working/train/giza.fr-en/fr-en.cooc -c /users/case4/wallm22/smt-lab/working/train/corpus/fr-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /users/case4/wallm22/smt-lab/working/train/giza.fr-en/fr-en -onlyaldumps 1 -p0 0.999 -s /users/case4/wallm22/smt-lab/working/train/corpus/en.vcb -t /users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb
/usr/local/mosesdecoder/tools/GIZA++  -CoocurrenceFile /users/case4/wallm22/smt-lab/working/train/giza.fr-en/fr-en.cooc -c /users/case4/wallm22/smt-lab/working/train/corpus/fr-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /users/case4/wallm22/smt-lab/working/train/giza.fr-en/fr-en -onlyaldumps 1 -p0 0.999 -s /users/case4/wallm22/smt-lab/working/train/corpus/en.vcb -t /users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb
Parameter 'coocurrencefile' changed from '' to '/users/case4/wallm22/smt-lab/working/train/giza.fr-en/fr-en.cooc'
Parameter 'c' changed from '' to '/users/case4/wallm22/smt-lab/working/train/corpus/fr-en-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '117-02-07.132323.wallm22' to '/users/case4/wallm22/smt-lab/working/train/giza.fr-en/fr-en'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/users/case4/wallm22/smt-lab/working/train/corpus/en.vcb'
Parameter 't' changed from '' to '/users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 117-02-07.132323.wallm22.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /users/case4/wallm22/smt-lab/working/train/giza.fr-en/fr-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /users/case4/wallm22/smt-lab/working/train/corpus/fr-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /users/case4/wallm22/smt-lab/working/train/corpus/en.vcb  (source vocabulary file name)
t = /users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 117-02-07.132323.wallm22.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /users/case4/wallm22/smt-lab/working/train/giza.fr-en/fr-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /users/case4/wallm22/smt-lab/working/train/corpus/fr-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /users/case4/wallm22/smt-lab/working/train/corpus/en.vcb  (source vocabulary file name)
t = /users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/users/case4/wallm22/smt-lab/working/train/corpus/en.vcb
Reading vocabulary file from:/users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb
Source vocabulary list has 2016 unique tokens 
Target vocabulary list has 2417 unique tokens 
Calculating vocabulary frequencies from corpus /users/case4/wallm22/smt-lab/working/train/corpus/fr-en-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 999 sentence pairs.
 Train total # sentence pairs (weighted): 999
Size of source portion of the training corpus: 10492 tokens
Size of the target portion of the training corpus: 11432 tokens 
In source portion of the training corpus, only 2015 unique tokens appeared
In target portion of the training corpus, only 2415 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 11432/(11491-999)== 1.08959
There are 79160 79160 entries in table
==========================================================
Model1 Training Started at: Tue Feb  7 13:23:23 2017

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 11.5231 PERPLEXITY 2943.05
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 15.2307 PERPLEXITY 38449.7
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 5.72673 PERPLEXITY 52.9563
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 7.77921 PERPLEXITY 219.672
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 5.25463 PERPLEXITY 38.177
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 6.89733 PERPLEXITY 119.207
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 5.01545 PERPLEXITY 32.3445
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 6.35268 PERPLEXITY 81.7235
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 4.89999 PERPLEXITY 29.8569
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 6.05889 PERPLEXITY 66.6664
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 0 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 2015  #classes: 51
Read classes: #words: 2416  #classes: 51

==========================================================
Hmm Training Started at: Tue Feb  7 13:23:23 2017

-----------
Hmm: Iteration 1
A/D table contains 18303 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.84116 PERPLEXITY 28.6639
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 5.8918 PERPLEXITY 59.3758

Hmm Iteration: 1 took: 0 seconds

-----------
Hmm: Iteration 2
A/D table contains 18303 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 4.36706 PERPLEXITY 20.6355
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 4.85579 PERPLEXITY 28.956

Hmm Iteration: 2 took: 0 seconds

-----------
Hmm: Iteration 3
A/D table contains 18303 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 3.75464 PERPLEXITY 13.4977
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 4.02261 PERPLEXITY 16.2527

Hmm Iteration: 3 took: 0 seconds

-----------
Hmm: Iteration 4
A/D table contains 18303 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 3.37655 PERPLEXITY 10.3859
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 3.55582 PERPLEXITY 11.76

Hmm Iteration: 4 took: 1 seconds

-----------
Hmm: Iteration 5
A/D table contains 18303 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 3.19641 PERPLEXITY 9.16676
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 3.33256 PERPLEXITY 10.074

Hmm Iteration: 5 took: 0 seconds

Entire Hmm Training took: 1 seconds
==========================================================
Read classes: #words: 2015  #classes: 51
Read classes: #words: 2416  #classes: 51
Read classes: #words: 2015  #classes: 51
Read classes: #words: 2416  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Tue Feb  7 13:23:24 2017


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 231.979 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 18303 parameters.
A/D table contains 18404 parameters.
NTable contains 20160 parameter.
p0_count is 10318.8 and p1 is 556.611; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 2.85943 PERPLEXITY 7.25728
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 2.93391 PERPLEXITY 7.6418

THTo3 Viterbi Iteration : 1 took: 0 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 232.071 #alsophisticatedcountcollection: 0 #hcsteps: 1.62663
#peggingImprovements: 0
A/D table contains 18303 parameters.
A/D table contains 18404 parameters.
NTable contains 20160 parameter.
p0_count is 10940.7 and p1 is 245.663; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.97454 PERPLEXITY 15.7201
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 4.02604 PERPLEXITY 16.2914

Model3 Viterbi Iteration : 2 took: 0 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 232.077 #alsophisticatedcountcollection: 0 #hcsteps: 1.6046
#peggingImprovements: 0
A/D table contains 18303 parameters.
A/D table contains 18399 parameters.
NTable contains 20160 parameter.
p0_count is 11105.8 and p1 is 163.113; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.81972 PERPLEXITY 14.1205
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 3.85803 PERPLEXITY 14.5005

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 232.059 #alsophisticatedcountcollection: 7.67568 #hcsteps: 1.6026
#peggingImprovements: 0
D4 table contains 412902 parameters.
A/D table contains 18303 parameters.
A/D table contains 18357 parameters.
NTable contains 20160 parameter.
p0_count is 11171.8 and p1 is 130.081; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 3.76193 PERPLEXITY 13.5661
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 3.79464 PERPLEXITY 13.8772

T3To4 Viterbi Iteration : 4 took: 0 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 232.069 #alsophisticatedcountcollection: 6.08609 #hcsteps: 1.50751
#peggingImprovements: 0
D4 table contains 412902 parameters.
A/D table contains 18303 parameters.
A/D table contains 18322 parameters.
NTable contains 20160 parameter.
p0_count is 11173.6 and p1 is 129.223; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.36904 PERPLEXITY 10.332
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 3.39108 PERPLEXITY 10.491

Model4 Viterbi Iteration : 5 took: 1 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 232.051 #alsophisticatedcountcollection: 5.06507 #hcsteps: 1.48749
#peggingImprovements: 0
D4 table contains 412902 parameters.
A/D table contains 18303 parameters.
A/D table contains 18265 parameters.
NTable contains 20160 parameter.
p0_count is 11207 and p1 is 112.504; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.28817 PERPLEXITY 9.76872
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 3.30517 PERPLEXITY 9.88448

Model4 Viterbi Iteration : 6 took: 0 seconds
H333444 Training Finished at: Tue Feb  7 13:23:25 2017


Entire Viterbi H333444 Training took: 1 seconds
==========================================================

Entire Training took: 2 seconds
Program Finished at: Tue Feb  7 13:23:25 2017

==========================================================
Executing: rm -f /users/case4/wallm22/smt-lab/working/train/giza.fr-en/fr-en.A3.final.gz
Executing: gzip /users/case4/wallm22/smt-lab/working/train/giza.fr-en/fr-en.A3.final
(2.1a) running snt2cooc en-fr @ Tue Feb  7 13:23:25 GMT 2017

Executing: mkdir -p /users/case4/wallm22/smt-lab/working/train/giza.en-fr
Executing: /usr/local/mosesdecoder/tools/snt2cooc.out /users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb /users/case4/wallm22/smt-lab/working/train/corpus/en.vcb /users/case4/wallm22/smt-lab/working/train/corpus/en-fr-int-train.snt > /users/case4/wallm22/smt-lab/working/train/giza.en-fr/en-fr.cooc
/usr/local/mosesdecoder/tools/snt2cooc.out /users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb /users/case4/wallm22/smt-lab/working/train/corpus/en.vcb /users/case4/wallm22/smt-lab/working/train/corpus/en-fr-int-train.snt > /users/case4/wallm22/smt-lab/working/train/giza.en-fr/en-fr.cooc
END.
(2.1b) running giza en-fr @ Tue Feb  7 13:23:25 GMT 2017
/usr/local/mosesdecoder/tools/GIZA++  -CoocurrenceFile /users/case4/wallm22/smt-lab/working/train/giza.en-fr/en-fr.cooc -c /users/case4/wallm22/smt-lab/working/train/corpus/en-fr-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /users/case4/wallm22/smt-lab/working/train/giza.en-fr/en-fr -onlyaldumps 1 -p0 0.999 -s /users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb -t /users/case4/wallm22/smt-lab/working/train/corpus/en.vcb
Executing: /usr/local/mosesdecoder/tools/GIZA++  -CoocurrenceFile /users/case4/wallm22/smt-lab/working/train/giza.en-fr/en-fr.cooc -c /users/case4/wallm22/smt-lab/working/train/corpus/en-fr-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /users/case4/wallm22/smt-lab/working/train/giza.en-fr/en-fr -onlyaldumps 1 -p0 0.999 -s /users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb -t /users/case4/wallm22/smt-lab/working/train/corpus/en.vcb
/usr/local/mosesdecoder/tools/GIZA++  -CoocurrenceFile /users/case4/wallm22/smt-lab/working/train/giza.en-fr/en-fr.cooc -c /users/case4/wallm22/smt-lab/working/train/corpus/en-fr-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /users/case4/wallm22/smt-lab/working/train/giza.en-fr/en-fr -onlyaldumps 1 -p0 0.999 -s /users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb -t /users/case4/wallm22/smt-lab/working/train/corpus/en.vcb
Parameter 'coocurrencefile' changed from '' to '/users/case4/wallm22/smt-lab/working/train/giza.en-fr/en-fr.cooc'
Parameter 'c' changed from '' to '/users/case4/wallm22/smt-lab/working/train/corpus/en-fr-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '117-02-07.132326.wallm22' to '/users/case4/wallm22/smt-lab/working/train/giza.en-fr/en-fr'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb'
Parameter 't' changed from '' to '/users/case4/wallm22/smt-lab/working/train/corpus/en.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 117-02-07.132326.wallm22.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /users/case4/wallm22/smt-lab/working/train/giza.en-fr/en-fr  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /users/case4/wallm22/smt-lab/working/train/corpus/en-fr-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb  (source vocabulary file name)
t = /users/case4/wallm22/smt-lab/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 117-02-07.132326.wallm22.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /users/case4/wallm22/smt-lab/working/train/giza.en-fr/en-fr  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /users/case4/wallm22/smt-lab/working/train/corpus/en-fr-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb  (source vocabulary file name)
t = /users/case4/wallm22/smt-lab/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/users/case4/wallm22/smt-lab/working/train/corpus/fr.vcb
Reading vocabulary file from:/users/case4/wallm22/smt-lab/working/train/corpus/en.vcb
Source vocabulary list has 2417 unique tokens 
Target vocabulary list has 2016 unique tokens 
Calculating vocabulary frequencies from corpus /users/case4/wallm22/smt-lab/working/train/corpus/en-fr-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 999 sentence pairs.
 Train total # sentence pairs (weighted): 999
Size of source portion of the training corpus: 11432 tokens
Size of the target portion of the training corpus: 10492 tokens 
In source portion of the training corpus, only 2416 unique tokens appeared
In target portion of the training corpus, only 2014 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 10492/(12431-999)== 0.917775
There are 78759 78759 entries in table
==========================================================
Model1 Training Started at: Tue Feb  7 13:23:26 2017

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 11.3034 PERPLEXITY 2527.33
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 15.1013 PERPLEXITY 35150.6
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 5.40213 PERPLEXITY 42.2866
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 7.56427 PERPLEXITY 189.266
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 4.91505 PERPLEXITY 30.1701
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 6.59484 PERPLEXITY 96.6598
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 4.66273 PERPLEXITY 25.3293
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 6.01471 PERPLEXITY 64.6558
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 4.54191 PERPLEXITY 23.2944
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 5.7139 PERPLEXITY 52.4875
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 0 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 2416  #classes: 51
Read classes: #words: 2015  #classes: 51

==========================================================
Hmm Training Started at: Tue Feb  7 13:23:26 2017

-----------
Hmm: Iteration 1
A/D table contains 18583 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.48138 PERPLEXITY 22.3372
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 5.54735 PERPLEXITY 46.7649

Hmm Iteration: 1 took: 0 seconds

-----------
Hmm: Iteration 2
A/D table contains 18583 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 3.92962 PERPLEXITY 15.2382
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 4.36864 PERPLEXITY 20.6582

Hmm Iteration: 2 took: 0 seconds

-----------
Hmm: Iteration 3
A/D table contains 18583 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 3.21366 PERPLEXITY 9.27703
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 3.42762 PERPLEXITY 10.7601

Hmm Iteration: 3 took: 0 seconds

-----------
Hmm: Iteration 4
A/D table contains 18583 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.83364 PERPLEXITY 7.12871
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.96992 PERPLEXITY 7.83494

Hmm Iteration: 4 took: 1 seconds

-----------
Hmm: Iteration 5
A/D table contains 18583 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.68018 PERPLEXITY 6.40934
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 2.78485 PERPLEXITY 6.89165

Hmm Iteration: 5 took: 0 seconds

Entire Hmm Training took: 1 seconds
==========================================================
Read classes: #words: 2416  #classes: 51
Read classes: #words: 2015  #classes: 51
Read classes: #words: 2416  #classes: 51
Read classes: #words: 2015  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Tue Feb  7 13:23:27 2017


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 218.449 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 18583 parameters.
A/D table contains 18078 parameters.
NTable contains 24170 parameter.
p0_count is 9767.1 and p1 is 362.451; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 2.30187 PERPLEXITY 4.93096
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 2.36487 PERPLEXITY 5.15107

THTo3 Viterbi Iteration : 1 took: 0 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 218.529 #alsophisticatedcountcollection: 0 #hcsteps: 1.53053
#peggingImprovements: 0
A/D table contains 18583 parameters.
A/D table contains 18078 parameters.
NTable contains 24170 parameter.
p0_count is 10182.3 and p1 is 154.829; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.40652 PERPLEXITY 10.6039
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 3.4561 PERPLEXITY 10.9747

Model3 Viterbi Iteration : 2 took: 0 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 218.538 #alsophisticatedcountcollection: 0 #hcsteps: 1.53754
#peggingImprovements: 0
A/D table contains 18583 parameters.
A/D table contains 17969 parameters.
NTable contains 24170 parameter.
p0_count is 10295 and p1 is 98.5149; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.27954 PERPLEXITY 9.71047
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 3.31519 PERPLEXITY 9.95342

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 218.534 #alsophisticatedcountcollection: 7.41642 #hcsteps: 1.47648
#peggingImprovements: 0
D4 table contains 415744 parameters.
A/D table contains 18583 parameters.
A/D table contains 17942 parameters.
NTable contains 24170 parameter.
p0_count is 10338 and p1 is 77.0009; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 3.2238 PERPLEXITY 9.34246
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 3.253 PERPLEXITY 9.53343

T3To4 Viterbi Iteration : 4 took: 0 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 218.548 #alsophisticatedcountcollection: 5.46346 #hcsteps: 1.36436
#peggingImprovements: 0
D4 table contains 415744 parameters.
A/D table contains 18583 parameters.
A/D table contains 17826 parameters.
NTable contains 24170 parameter.
p0_count is 10350.8 and p1 is 70.6031; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.68596 PERPLEXITY 6.43507
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 2.70393 PERPLEXITY 6.51577

Model4 Viterbi Iteration : 5 took: 1 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 218.552 #alsophisticatedcountcollection: 4.6046 #hcsteps: 1.33534
#peggingImprovements: 0
D4 table contains 415744 parameters.
A/D table contains 18583 parameters.
A/D table contains 17691 parameters.
NTable contains 24170 parameter.
p0_count is 10362.7 and p1 is 64.6429; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 2.61458 PERPLEXITY 6.12444
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 2.62979 PERPLEXITY 6.18938

Model4 Viterbi Iteration : 6 took: 0 seconds
H333444 Training Finished at: Tue Feb  7 13:23:28 2017


Entire Viterbi H333444 Training took: 1 seconds
==========================================================

Entire Training took: 2 seconds
Program Finished at: Tue Feb  7 13:23:28 2017

==========================================================
Executing: rm -f /users/case4/wallm22/smt-lab/working/train/giza.en-fr/en-fr.A3.final.gz
Executing: gzip /users/case4/wallm22/smt-lab/working/train/giza.en-fr/en-fr.A3.final
(3) generate word alignment @ Tue Feb  7 13:23:28 GMT 2017
Combining forward and inverted alignment from files:
  /users/case4/wallm22/smt-lab/working/train/giza.fr-en/fr-en.A3.final.{bz2,gz}
  /users/case4/wallm22/smt-lab/working/train/giza.en-fr/en-fr.A3.final.{bz2,gz}
Executing: mkdir -p /users/case4/wallm22/smt-lab/working/train/model
Executing: /usr/local/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /users/case4/wallm22/smt-lab/working/train/giza.en-fr/en-fr.A3.final.gz" -i "gzip -cd /users/case4/wallm22/smt-lab/working/train/giza.fr-en/fr-en.A3.final.gz" |/usr/local/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /users/case4/wallm22/smt-lab/working/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<999>
(4) generate lexical translation table 0-0 @ Tue Feb  7 13:23:29 GMT 2017
(/users/case4/wallm22/smt-lab/corpus/train-data-clean.fr,/users/case4/wallm22/smt-lab/corpus/train-data-clean.en,/users/case4/wallm22/smt-lab/working/train/model/lex)
!
Saved: /users/case4/wallm22/smt-lab/working/train/model/lex.f2e and /users/case4/wallm22/smt-lab/working/train/model/lex.e2f
FILE: /users/case4/wallm22/smt-lab/corpus/train-data-clean.en
FILE: /users/case4/wallm22/smt-lab/corpus/train-data-clean.fr
FILE: /users/case4/wallm22/smt-lab/working/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Tue Feb  7 13:23:29 GMT 2017
/usr/local/mosesdecoder/scripts/generic/extract-parallel.perl 1 split "sort    " /usr/local/mosesdecoder/scripts/../bin/extract /users/case4/wallm22/smt-lab/corpus/train-data-clean.en /users/case4/wallm22/smt-lab/corpus/train-data-clean.fr /users/case4/wallm22/smt-lab/working/train/model/aligned.grow-diag-final-and /users/case4/wallm22/smt-lab/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /usr/local/mosesdecoder/scripts/generic/extract-parallel.perl 1 split "sort    " /usr/local/mosesdecoder/scripts/../bin/extract /users/case4/wallm22/smt-lab/corpus/train-data-clean.en /users/case4/wallm22/smt-lab/corpus/train-data-clean.fr /users/case4/wallm22/smt-lab/working/train/model/aligned.grow-diag-final-and /users/case4/wallm22/smt-lab/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Tue Feb  7 13:23:29 2017
Executing: ln -s /users/case4/wallm22/smt-lab/corpus/train-data-clean.en /users/case4/wallm22/smt-lab/working/train/model/tmp.24073/target.00000 
total=999 line-per-split=1000 
Executing: ln -s /users/case4/wallm22/smt-lab/corpus/train-data-clean.fr /users/case4/wallm22/smt-lab/working/train/model/tmp.24073/source.00000 
Executing: ln -s /users/case4/wallm22/smt-lab/working/train/model/aligned.grow-diag-final-and /users/case4/wallm22/smt-lab/working/train/model/tmp.24073/align.00000 
/usr/local/mosesdecoder/scripts/../bin/extract /users/case4/wallm22/smt-lab/working/train/model/tmp.24073/target.00000 /users/case4/wallm22/smt-lab/working/train/model/tmp.24073/source.00000 /users/case4/wallm22/smt-lab/working/train/model/tmp.24073/align.00000 /users/case4/wallm22/smt-lab/working/train/model/tmp.24073/extract.00000  7 orientation --model wbe-msd --GZOutput   --SentenceOffset 0 2>> /dev/stderr 
glueArg= 
merging extract / extract.inv
gunzip -c /users/case4/wallm22/smt-lab/working/train/model/tmp.24073/extract.00000.o.gz  | LC_ALL=C sort     -T /users/case4/wallm22/smt-lab/working/train/model/tmp.24073 2>> /dev/stderr | gzip -c > /users/case4/wallm22/smt-lab/working/train/model/extract.o.sorted.gz 2>> /dev/stderr 
rm -rf /users/case4/wallm22/smt-lab/working/train/model/tmp.24073 
Finished Tue Feb  7 13:23:30 2017
(6) score phrases @ Tue Feb  7 13:23:30 GMT 2017
(6.1)  creating table half /users/case4/wallm22/smt-lab/working/train/model/phrase-table.half.f2e @ Tue Feb  7 13:23:30 GMT 2017
/usr/local/mosesdecoder/scripts/generic/score-parallel.perl 1 "sort    " /usr/local/mosesdecoder/scripts/../bin/score /users/case4/wallm22/smt-lab/working/train/model/extract.sorted.gz /users/case4/wallm22/smt-lab/working/train/model/lex.f2e /users/case4/wallm22/smt-lab/working/train/model/phrase-table.half.f2e.gz  0 
Executing: /usr/local/mosesdecoder/scripts/generic/score-parallel.perl 1 "sort    " /usr/local/mosesdecoder/scripts/../bin/score /users/case4/wallm22/smt-lab/working/train/model/extract.sorted.gz /users/case4/wallm22/smt-lab/working/train/model/lex.f2e /users/case4/wallm22/smt-lab/working/train/model/phrase-table.half.f2e.gz  0 
Started Tue Feb  7 13:23:30 2017
ln -s /users/case4/wallm22/smt-lab/working/train/model/extract.sorted.gz /users/case4/wallm22/smt-lab/working/train/model/tmp.24104/extract.0.gz 
/usr/local/mosesdecoder/scripts/../bin/score /users/case4/wallm22/smt-lab/working/train/model/tmp.24104/extract.0.gz /users/case4/wallm22/smt-lab/working/train/model/lex.f2e /users/case4/wallm22/smt-lab/working/train/model/tmp.24104/phrase-table.half.00000.gz  2>> /dev/stderr 
/users/case4/wallm22/smt-lab/working/train/model/tmp.24104/run.0.shmv /users/case4/wallm22/smt-lab/working/train/model/tmp.24104/phrase-table.half.00000.gz /users/case4/wallm22/smt-lab/working/train/model/phrase-table.half.f2e.gzrm -rf /users/case4/wallm22/smt-lab/working/train/model/tmp.24104 
Finished Tue Feb  7 13:23:31 2017
(6.3)  creating table half /users/case4/wallm22/smt-lab/working/train/model/phrase-table.half.e2f @ Tue Feb  7 13:23:31 GMT 2017
/usr/local/mosesdecoder/scripts/generic/score-parallel.perl 1 "sort    " /usr/local/mosesdecoder/scripts/../bin/score /users/case4/wallm22/smt-lab/working/train/model/extract.inv.sorted.gz /users/case4/wallm22/smt-lab/working/train/model/lex.e2f /users/case4/wallm22/smt-lab/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /usr/local/mosesdecoder/scripts/generic/score-parallel.perl 1 "sort    " /usr/local/mosesdecoder/scripts/../bin/score /users/case4/wallm22/smt-lab/working/train/model/extract.inv.sorted.gz /users/case4/wallm22/smt-lab/working/train/model/lex.e2f /users/case4/wallm22/smt-lab/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Started Tue Feb  7 13:23:31 2017
ln -s /users/case4/wallm22/smt-lab/working/train/model/extract.inv.sorted.gz /users/case4/wallm22/smt-lab/working/train/model/tmp.24115/extract.0.gz 
/usr/local/mosesdecoder/scripts/../bin/score /users/case4/wallm22/smt-lab/working/train/model/tmp.24115/extract.0.gz /users/case4/wallm22/smt-lab/working/train/model/lex.e2f /users/case4/wallm22/smt-lab/working/train/model/tmp.24115/phrase-table.half.00000.gz --Inverse  2>> /dev/stderr 
/users/case4/wallm22/smt-lab/working/train/model/tmp.24115/run.0.shgunzip -c /users/case4/wallm22/smt-lab/working/train/model/tmp.24115/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /users/case4/wallm22/smt-lab/working/train/model/tmp.24115  | gzip -c > /users/case4/wallm22/smt-lab/working/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /users/case4/wallm22/smt-lab/working/train/model/tmp.24115 
Finished Tue Feb  7 13:23:33 2017
(6.6) consolidating the two halves @ Tue Feb  7 13:23:33 GMT 2017
Executing: /usr/local/mosesdecoder/scripts/../bin/consolidate /users/case4/wallm22/smt-lab/working/train/model/phrase-table.half.f2e.gz /users/case4/wallm22/smt-lab/working/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /users/case4/wallm22/smt-lab/working/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables
Executing: rm -f /users/case4/wallm22/smt-lab/working/train/model/phrase-table.half.*
(7) learn reordering model @ Tue Feb  7 13:23:33 GMT 2017
(7.1) [no factors] learn reordering model @ Tue Feb  7 13:23:33 GMT 2017
(7.2) building tables @ Tue Feb  7 13:23:33 GMT 2017
Executing: /usr/local/mosesdecoder/scripts/../bin/lexical-reordering-score /users/case4/wallm22/smt-lab/working/train/model/extract.o.sorted.gz 0.5 /users/case4/wallm22/smt-lab/working/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Tue Feb  7 13:23:34 GMT 2017
  no generation model requested, skipping step
(9) create moses.ini @ Tue Feb  7 13:23:34 GMT 2017
